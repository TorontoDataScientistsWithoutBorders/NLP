{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we implement a simple Deep Learning dependency parser following this [paper](http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf) and the [homework](http://web.stanford.edu/class/cs224n/assignment2/index.html) from the Standford NLP course. \n",
    "\n",
    "We recommend that you read throught this [notes](http://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-notes4.pdf) and the [Wiki webpage](https://en.wikipedia.org/wiki/Dependency_grammar) before continuing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a dependency parser?\n",
    "\n",
    "Dependency grammar relies on the assumption that in any sentence every word is related in an asymmetric way with at most another word.\n",
    "\n",
    "We call these relations dependencies. In a general sense we say that a word $w_1$ is dependend of a word $w_2$ ($w_1$->$w_2$) if word $w_1$ modifies word $w_2$.  \n",
    "\n",
    "**Every sentence can be parsed in a unique way!**\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "How should we parse the sentence \"I saw an ice-cream with my phone\"?\n",
    "\n",
    "'saw'-> 'I'\n",
    "\n",
    "'saw'-> 'ice-cream'\n",
    "\n",
    "'saw'-> 'with'\n",
    "\n",
    "'ice-cream' -> 'an'\n",
    "\n",
    "'with' -> 'phone'\n",
    "\n",
    "'phone'-> 'my'\n",
    "\n",
    "\n",
    "How can we build our own parser?\n",
    "\n",
    "\n",
    "## Greedy Transition-Based parsing\n",
    "\n",
    "Thanks to the assumptions on the parsing structure we can build the parsing in a number of steps, to do this we need some auxiliary structure. We explain this next. \n",
    "\n",
    "Let $S=w_0w_1\\cdots w_n$ be a sentence, we consider the triple $$ c=(\\gamma,\\beta, A),$$ where\n",
    "\n",
    "- A stach $\\gamma$ of words from $S$.\n",
    "- A buffer $\\beta$ of words from $S$.\n",
    "- A **set** of arcs $A$ of the form $(w_i,w_j)$.\n",
    "\n",
    "The **goal** is to start from the inital state\n",
    "\n",
    "$$c_0=\\left(\\gamma_0=['ROOT'],\\beta_0=[w_0,w_1,\\ldots,w_n],A_0=\\emptyset\\right)$$\n",
    "\n",
    "and use the transitions \n",
    "\n",
    "- **Shift**: Remove first words in $\\beta$ and push it on top of $\\gamma$.\n",
    "- **Left-Arc**: Add $(w_j,w_i)$ to $A$, where $w_i$ is the second to the top word on $\\gamma$ and $w_j$ is the word at the top. Then, remove $w_i$ from $\\gamma$.\n",
    "- **Right-Arc**: Add $(w_i,w_j)$ to $A$, where $w_i$ is the second to the top word on $\\gamma$ and $w_j$ is the word at the top. Then, remove $w_j$ from $\\gamma$.\n",
    "\n",
    "to achieve a configuration\n",
    "\n",
    "$$c_n=\\left(\\gamma_n=['ROOT'],\\beta_n=\\emptyset,A\\right)$$\n",
    "\n",
    "where A_n is now a parsing structure for the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PartialParse Object\n",
    "\n",
    "We start by creating an object to keep the different configurations that we obtain from c_i, we call that object a PartialParse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Initializes this partial parse.\n",
    "            We initialize the following fields:\n",
    "            self.stack: The current stack represented as a list with the top of the stack as the\n",
    "                        last element of the list.\n",
    "            self.buffer: The current buffer represented as a list with the first item on the\n",
    "                         buffer as the first item of the list\n",
    "            self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "                    tuples where each tuple is of the form (head, dependent).\n",
    "                    Order for this list doesn't matter.\n",
    "\n",
    "        The root token should be represented with the string \"ROOT\"\n",
    "\n",
    "        Args:\n",
    "            sentence: The sentence to be parsed as a list of words.\n",
    "                      Your code should not modify the sentence.\n",
    "        \"\"\"\n",
    "        # The sentence being parsed is kept for bookkeeping purposes.\n",
    "        \n",
    "        self.sentence = sentence\n",
    "        self.stack=['ROOT']\n",
    "        self.buffer=sentence\n",
    "        self.dependencies=[]\n",
    "       \n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
    "\n",
    "        Args:\n",
    "            transition: A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc,\n",
    "                        and right-arc transitions.\n",
    "        \"\"\"\n",
    "        \n",
    "        if transition==\"S\":\n",
    "            # Removes the first one from the buffer\n",
    "            b1=self.buffer.pop(0)\n",
    "            # adds it to the stack\n",
    "            self.stack.append(b1)\n",
    "            \n",
    "        elif transition==\"LA\":\n",
    "            #removes the second one from the stack\n",
    "            s2=self.stack.pop(-2)\n",
    "            \n",
    "            s1=self.stack[-1]\n",
    "            #adds the arc s1->s2 to the dependencies.\n",
    "            self.dependencies.append((s1,s2))\n",
    "            \n",
    "        elif transition==\"RA\":\n",
    "            #removes the first one from the stack\n",
    "            s1=self.stack.pop(-1)\n",
    "            \n",
    "            s2=self.stack[-1]\n",
    "            #adds the arc s2->s1 to the dependencies.\n",
    "            self.dependencies.append((s2,s1))\n",
    "\n",
    "\n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "\n",
    "        Args:\n",
    "            transitions: The list of transitions in the order they should be applied\n",
    "        Returns:\n",
    "            dependencies: The list of dependencies produced when parsing the sentence. Represented\n",
    "                          as a list of tuples where each tuple is of the form (head, dependent)\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example if we have the sentence \"I saw a cow\", we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('saw', 'I'), ('cow', 'a'), ('saw', 'cow')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pp=PartialParse(['I','saw','a','cow'])\n",
    "example_pp.parse(['S','S','LA','S','S','LA','RA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Check the transitions that are necessary to give a correct parsing of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Deep Learning stuff\n",
    "\n",
    "In order to create such a parser we follow the approach of assigment 2. That is, we want to create a NN that for a given triple $c$ decides what is the next transition. \n",
    "\n",
    "We are given some training data, for our purposes the data looks like a collection of tables like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|index|word|arrow from|\n",
    "|---|---|---|\n",
    "|1|Ms.|2|\n",
    "|2|Haag|3|\n",
    "|3|plays|0|\n",
    "|4|Elianti|3|\n",
    "|5|.|3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it will be preprocessed so it looks in the form X,y. We will go over the generalities of this during the lecture. But keep in mind that a greedy approach in the selection of the correct transitions make the desired steps unique. \n",
    "\n",
    "In the homework, the method that gives the data is load_and preprocess_data inside the parser_utils module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from assignment2_sol.utils.parser_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "took 1.90 seconds\n",
      "Building parser...\n",
      "took 0.03 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 2.19 seconds\n",
      "Vectorizing data...\n",
      "took 0.07 seconds\n",
      "Preprocessing training data...\n",
      "1000/1000 [==============================] - 3s     \n"
     ]
    }
   ],
   "source": [
    "data=load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we obtain 5 different objects, we only care about the second one which is the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_matrix=data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the third one who is the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "furthermore, the training data comes in the form of a list which each elemnt is (x,something,y), so we collect the x and the y parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_input_data=np.array([train_data_element[0] for train_data_element in train_data])\n",
    "y_input_data_=np.array([train_data_element[2] for train_data_element in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need one more preprocessing since the y_input_data is not exactly in the shape we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_input_data = np.zeros((y_input_data_.size, 3))\n",
    "y_input_data[np.arange(y_input_data_.size), y_input_data_] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to build our model, first some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 36\n",
    "n_classes = 3\n",
    "dropout = 0.5\n",
    "embed_size = 50\n",
    "hidden_size = 200\n",
    "batch_size = 2048\n",
    "n_epochs = 10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we start with the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_placeholder=tf.placeholder(shape=(None,n_features),dtype=tf.int32,name='input_placeholder')\n",
    "labels_placeholder=tf.placeholder(shape=(None,n_classes),dtype=tf.float32,name='labels_placeholder')\n",
    "dropout_placeholder=tf.placeholder(shape=(),dtype=tf.float32,name='dropout_rate_placeholder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a dictionary to feed the data to the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict={input_placeholder: x_input_data,\n",
    "           labels_placeholder:y_input_data,\n",
    "           dropout_placeholder: dropout}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings=tf.Variable(emb_matrix)\n",
    "embeddings= tf.nn.embedding_lookup(embeddings,input_placeholder)\n",
    "embeddings= tf.reshape(embeddings,shape=(-1,embed_size*n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then the prediction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W=tf.Variable(tf.random_normal(shape=(n_features*embed_size,hidden_size)))\n",
    "b1=tf.Variable(tf.zeros(hidden_size))\n",
    "U = tf.Variable(tf.random_normal(shape=(hidden_size,n_classes)))\n",
    "b2=tf.Variable(tf.zeros(n_classes))\n",
    "\n",
    "pred = tf.nn.relu(tf.matmul(embeddings,W)+b1)\n",
    "\n",
    "pred = tf.nn.dropout(pred,dropout)\n",
    "\n",
    "pred = tf.matmul(pred,U)+b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=labels_placeholder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and something to measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = tf.argmax(pred, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(labels_placeholder, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the training op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op=tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is step: 99, acc=0.70, loss=53.010\r"
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "for i in range(100):\n",
    "    acc,loss_,_=sess.run([accuracy,loss,train_op],feed_dict=feed_dict)\n",
    "    losses.append(loss_)\n",
    "    print(\"This is step: %d, acc=%.2f, loss=%.2f\"%(i,acc,loss_),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[365.65451,\n",
       " 348.58734,\n",
       " 334.02246,\n",
       " 322.79794,\n",
       " 316.72318,\n",
       " 308.40723,\n",
       " 300.366,\n",
       " 292.4176,\n",
       " 284.65366,\n",
       " 279.12256,\n",
       " 270.18951,\n",
       " 262.21478,\n",
       " 255.6797,\n",
       " 249.67032,\n",
       " 241.34106,\n",
       " 233.1259,\n",
       " 228.56071,\n",
       " 220.80252,\n",
       " 218.73036,\n",
       " 214.23166,\n",
       " 207.27528,\n",
       " 201.81181,\n",
       " 200.13739,\n",
       " 193.41034,\n",
       " 189.15839,\n",
       " 184.00226,\n",
       " 179.60867,\n",
       " 175.61424,\n",
       " 171.13467,\n",
       " 168.79919,\n",
       " 164.05524,\n",
       " 159.63693,\n",
       " 155.03279,\n",
       " 153.07359,\n",
       " 151.05206,\n",
       " 148.49324,\n",
       " 145.43581,\n",
       " 142.39102,\n",
       " 140.13947,\n",
       " 135.82745,\n",
       " 134.04579,\n",
       " 131.13884,\n",
       " 129.32338,\n",
       " 125.17753,\n",
       " 123.28595,\n",
       " 121.03934,\n",
       " 118.26649,\n",
       " 115.74667,\n",
       " 115.36729,\n",
       " 113.51044,\n",
       " 110.70678,\n",
       " 108.23536,\n",
       " 105.79965,\n",
       " 105.04689,\n",
       " 104.61577,\n",
       " 100.49763,\n",
       " 98.619942,\n",
       " 96.990105,\n",
       " 95.667763,\n",
       " 94.398201,\n",
       " 93.85051,\n",
       " 91.468277,\n",
       " 88.427483,\n",
       " 87.729851,\n",
       " 86.674782,\n",
       " 85.966019,\n",
       " 84.664902,\n",
       " 82.334122,\n",
       " 80.667542,\n",
       " 80.998085,\n",
       " 77.860703,\n",
       " 77.353546,\n",
       " 75.913528,\n",
       " 75.011467,\n",
       " 74.212814,\n",
       " 72.594559,\n",
       " 71.69545,\n",
       " 71.56852,\n",
       " 70.517609,\n",
       " 69.771446,\n",
       " 68.564003,\n",
       " 67.647987,\n",
       " 65.950577,\n",
       " 64.516777,\n",
       " 64.398285,\n",
       " 63.481026,\n",
       " 62.591537,\n",
       " 61.646168,\n",
       " 60.298439,\n",
       " 59.144276,\n",
       " 58.883518,\n",
       " 57.374573,\n",
       " 56.78059,\n",
       " 56.732269,\n",
       " 55.34964,\n",
       " 54.595551,\n",
       " 53.629242,\n",
       " 53.214523,\n",
       " 52.470066,\n",
       " 53.009987]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
