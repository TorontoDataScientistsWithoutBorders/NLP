{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of thsi notebook is to implement a simple text generation using LSTM's and Tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "We will be using a bunch of books from the Gutenberg website, this will be located in the folder data. The data will be proccess via the library utils, more specifically the method process_data() there will take care of getting the data ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate what it does let's assume that our goal is to look at the last 5 characters and deduce the next so to create text and that the whole book consist of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus=\"This is the whole book, it really doesn't have tons of stuff, but it should work fine for a simpel example.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want first to create token for each character, this can be done easily in many ways, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=sorted(set(corpus))\n",
    "char_to_token={char:i for i,char in enumerate(vocab)}\n",
    "token_to_char={i:char for i,char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can count the characters by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the tokens associated to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 10, 11, 19, 0, 11, 19, 0, 20, 10, 8, 0, 23, 10, 16, 13, 8, 0, 6, 16, 16, 12, 2, 0, 11, 20, 0, 18, 8, 5, 13, 13, 25, 0, 7, 16, 8, 19, 15, 1, 20, 0, 10, 5, 22, 8, 0, 20, 16, 15, 19, 0, 16, 9, 0, 19, 20, 21, 9, 9, 2, 0, 6, 21, 20, 0, 11, 20, 0, 19, 10, 16, 21, 13, 7, 0, 23, 16, 18, 12, 0, 9, 11, 15, 8, 0, 9, 16, 18, 0, 5, 0, 19, 11, 14, 17, 8, 13, 0, 8, 24, 5, 14, 17, 13, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "tokens_data=[char_to_token[char] for char in corpus]\n",
    "print(tokens_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we want to have sequences of five elements we break the tokens_data into lenghts of five "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 10, 11, 19, 0], [10, 11, 19, 0, 11], [11, 19, 0, 11, 19], [19, 0, 11, 19, 0], [0, 11, 19, 0, 20]]\n",
      "\n",
      "There are 102 sequences in the corpus.\n"
     ]
    }
   ],
   "source": [
    "tokens_data_seq=[tokens_data[i:i+5] for i in range(len(tokens_data)-5)]\n",
    "print(tokens_data_seq[:5])\n",
    "print(\"\\nThere are %d sequences in the corpus.\"%len(tokens_data_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the X and y values from this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=tokens_data_seq[:-1]\n",
    "y=tokens_data_seq[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can make two batches out of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batches=np.array([X[:50],X[50:-1]])\n",
    "y_batches=np.array([y[:50],y[50:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, there are two batches of 50 sequences each.This, together with the dicts, is (almost) what the method process_data returns. We now import Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with the data we are ready to build our graph, as always we start with the placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens=tf.placeholder(shape=(None,5), dtype=tf.float32,name='input_tokens')\n",
    "output_tokens=tf.placeholder(shape=(None,5),dtype=tf.int32,name='output_tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be feed to a RNN made of LSTM's, hence the first step \n",
    "\n",
    "<img src=\"lstmRNN.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Recurrent_layers\"):\n",
    "    \n",
    "    lstms=[tf.contrib.rnn.LSTMCell(100) for i in range(3)]\n",
    "    staked_lstm = tf.contrib.rnn.MultiRNNCell(lstms)\n",
    "    initial_state=state=staked_lstm.zero_state(50,dtype=tf.float32)\n",
    "    \n",
    "    #we must reshape the inputs since RNN take vectors not scalars\n",
    "    \n",
    "    output = tf.reshape(input_tokens, (-1,5,1),name=\"reshaped_input\")\n",
    "    output,_ = tf.nn.dynamic_rnn(staked_lstm,output,dtype=tf.float32)\n",
    "    \n",
    "    #now the output is a rank 3 tensor, \n",
    "    #we want to make it a matrix so we can use a softmax layer\n",
    "    \n",
    "    output = tf.reshape(output,[-1,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we create the softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Softmax_layer\"):\n",
    "    W = tf.Variable(tf.random_normal(shape=(100,26)))\n",
    "    b= tf.Variable(tf.zeros(26))\n",
    "    \n",
    "    output = tf.matmul(output,W)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the lack of activation function in this layer, we incorporated this inside the loss op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    one_hot=tf.nn.embedding_lookup(tf.eye(26),ids=output_tokens)\n",
    "    loss=tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=output,labels=one_hot\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "let's use Adam optimizer for the gradient descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimizer\"):\n",
    "    global_step=tf.Variable(0,dtype=tf.int32,trainable=False,name=\"global_step\")\n",
    "    optimizer=tf.train.AdamOptimizer().minimize(loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as we want to be able to see how we are doing, we create some summary operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"summaries\"):\n",
    "    loss_summary =tf.summary.scalar(\"Loss\",loss)\n",
    "    loss_histogram = tf.summary.histogram(\"hist_loss\",loss)\n",
    "    summary_op=tf.summary.merge((loss_summary,loss_histogram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are ready for the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./graph_notebook\",sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run it for a number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200\r"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    print(\"Epoch %d\"%(i+1), end=\"\\r\")\n",
    "    for X_batch,y_batch in zip(X_batches,y_batches):\n",
    "        feed_dict={input_tokens:X_batch,output_tokens:y_batch}\n",
    "        _,summary=sess.run([optimizer,summary_op],feed_dict=feed_dict)\n",
    "        writer.add_summary(summary,global_step=sess.run(global_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see how we did, to check on that we need to create a story, we start with 5 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "story='this '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, we add a character at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_tokens=[char_to_token[x] for x in story]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now we create a loop for finding say 200 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    X=[story_tokens[-5:]]\n",
    "    story_tokens.append(np.argmax(sess.run(output,feed_dict={input_tokens:X})[-1]))\n",
    "story=''.join([token_to_char[i] for i in story_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it really doesn't have tons of stuff, but it the whole book, it re\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Modify the code so it works for some other text, or any text."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
