{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we code a vanilla version of CBOW using Tensorflow and Oscar Wilde's 'The Picture of Dorian Gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to break the data into words, we are going to do it in a slightly fancier way using a tokenizer from the nltk package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book=open('./data/dorian.txt','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and create the list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_list=word_tokenize(book.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['may', 'copy', 'it', ',', 'give', 'it']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list[34:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the differences, we obtained more than just the words. Let's create the list of distinct tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8238"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(set(token_list))\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, there are 8238 different tokens. Now, we need to create our training data, recall that we are using CBOW, so we need to fix an $m$, for this post we use $m=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data_words=[]\n",
    "y_data_words=[]\n",
    "for i in range(2,len(token_list)-2):\n",
    "    X_data_words+=[[token_list[i-2],token_list[i-1],token_list[i+1],token_list[i+2]]]\n",
    "    y_data_words+=[[token_list[i]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we are ready to create the pipeline. Let's recall: \n",
    "- First it is one-hot encoding. \n",
    "- Embedding given by U.\n",
    "- Average.\n",
    "- Decoding via V.\n",
    "- Softmax\n",
    "- Loss function.\n",
    "\n",
    "## One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_enc(word):\n",
    "    one_hot=np.zeros(8238)\n",
    "    one_hot[tokens.index(word)]=1\n",
    "    return one_hot\n",
    "\n",
    "def one_hot_dec(vector):\n",
    "    return tokens[np.argmax(vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some further preparations:\n",
    "Unfortunately, we need to use a slightly different structure, the reason is that if we were to hold the one-hot encoding of every word in the X_data_words we will create a numpy array of size (98379,4,8238), and if we try to create an empyt such np array, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8bad72e21c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m98379\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8238\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.zeros((98379,4,8238))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is not a good sign, instead we use a different approach, we collect only the indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = np.array([[tokens.index(word) for word in _set] for _set in X_data_words])\n",
    "y_data = np.array([[tokens.index(word) for word in _set] for _set in y_data_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use tf.nn.embedding_lookup. (This is similar as how sparse matrices work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tensorflow Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create the objects that will appear on the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=(4),dtype=tf.int32,name='INPUT')\n",
    "y_ = tf.placeholder(shape=(1),dtype=tf.int32,name='OUTPUT')\n",
    "U = tf.Variable(tf.random_normal([8238,128], stddev=0.5),name=\"U\",dtype=tf.float32)\n",
    "V = tf.Variable(tf.random_normal([128,8238], stddev=0.5),name=\"V\",dtype=tf.float32)\n",
    "aveg_creator = tf.constant([[0.25,0.25,0.25,0.25]],name='average_creator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u= tf.nn.embedding_lookup(U,X)\n",
    "u_aveg= tf.matmul(aveg_creator,u,name='average')\n",
    "v= tf.matmul(u_aveg,V)\n",
    "y_hat = tf.nn.softmax(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss function and training\n",
    "\n",
    "We need to modify the loss function a bit for technical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=tf.nn.embedding_lookup(np.eye(8238,dtype=np.float32),y_)\n",
    "loss = -tf.reduce_sum(y*tf.log(tf.clip_by_value(y_hat,1e-10,1.0)))+tf.nn.l2_loss(U)+tf.nn.l2_loss(V)\n",
    "#loss = -tf.reduce_sum(y * tf.log(y_hat), reduction_indices=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use AdamOptimizer for the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=tf.train.AdamOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline\n",
    "We are ready to start the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run 100000 times, note that this is quite little for this data set because of the way we build it. Why, there are 80000 examples and we are only going over then one at the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0  the loss is  190043.0\n",
      "step:  1000  the loss is  9.01697\n",
      "step:  2000  the loss is  10.591\n",
      "step:  3000  the loss is  11.8745\n",
      "step:  4000  the loss is  12.7963\n",
      "step:  5000  the loss is  15.0773\n",
      "step:  6000  the loss is  15.8069\n",
      "step:  7000  the loss is  19.2467\n",
      "step:  8000  the loss is  17.6352\n",
      "step:  9000  the loss is  18.9454\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    j=np.random.randint(98379)\n",
    "    sess.run(train,feed_dict={X:X_data[j],y_:y_data[j]})\n",
    "    if i%1000==0:\n",
    "        print('step: ',i,' the loss is ',sess.run(loss,feed_dict={X:X_data[j],y_:y_data[j]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5560, 7075, 448, 7332]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ex=['no', 'artist','ethical', 'sympathies']\n",
    "Ex_indexes=[tokens.index(word) for word in Ex]\n",
    "Ex_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugs\n",
      "clever\n",
      "examining\n",
      "again\n",
      "water-lilies\n",
      "oils\n",
      "my\n",
      "Bruno\n",
      "house-party\n",
      "crime\n"
     ]
    }
   ],
   "source": [
    "vec=sess.run(y_hat,feed_dict={X:Ex_indexes})\n",
    "for i in range(10):\n",
    "    print(tokens[vec[0].argsort()[-10:][::-1][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This word doesn't fit much in the middle, if you let it running for a couple hours it may do better, but don't expect much of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** This is by far not the best way to do this. Here are some things to note and improve:\n",
    "\n",
    "- We are using an on-line gradient descent, similar to batch with small batches (size 1). This is slow but easier to code.\n",
    "- The loss function we are using is not the best option, there is a softmax_cross_entropy that would allow to avoid the cliping of the values and the regularizers.\n",
    "- The data feeding can be improve.\n",
    "- The network should be designed to recieve batches, we are fixing the entry values.\n",
    "- Checking the loss in this way is not a great idea, better use tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework:** Learn to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Keras\n",
    "\n",
    "We can also use Keras to create the model, first we need to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_K= Sequential()\n",
    "cbow_K.add(Embedding(input_dim=8238,output_dim=128,input_length=4))\n",
    "cbow_K.add(Lambda(lambda x:K.mean(x,axis=1),output_shape=(128,)))\n",
    "cbow_K.add(Dense(8238,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cbow_K.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration loss: 5.340\n",
      "1000 iteration loss: 5010.296\n",
      "2000 iteration loss: 10750.323\n",
      "3000 iteration loss: 16300.109\n",
      "4000 iteration loss: 22115.803\n",
      "5000 iteration loss: 27895.694\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    loss=0\n",
    "    for i in range(len(X_data)):\n",
    "        one_hot=np.zeros(8238)\n",
    "        one_hot[y_data[i]]=1\n",
    "        one_hot=np.array([one_hot])\n",
    "        XXX=np.array([X_data[i]])\n",
    "        loss+=cbow_K.train_on_batch(XXX,one_hot)\n",
    "        if i%1000==0:\n",
    "            print(\"%d iteration\"%i,\"loss: %.3f\"%loss)\n",
    "        if i>5000:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=cbow_K.predict(np.array([Ex_indexes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      ".\n",
      ",\n",
      "of\n",
      "is\n",
      "a\n",
      "to\n",
      "in\n",
      "and\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokens[vec[0].argsort()[-10:][::-1][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
