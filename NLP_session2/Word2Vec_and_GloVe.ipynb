{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we learn how to use w2v, and the pretrained models obtained from GloVe. \n",
    "\n",
    "**Remark:** If you want to train your own GloVe model you should either use one of the wrapers that are for Python (I haven't worked with them so I can not tell you how bug free they are) or train using their implementation. See their [Github](https://github.com/stanfordnlp/GloVe) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is part of the gensim package, and it is only one of many possible embeddings. You can find the documentation [here](https://radimrehurek.com/gensim/models/word2vec.html) We import the class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.word2vec as w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a model we need to feed it a list of sentences, of importance for us within the signature are the the \n",
    "\n",
    "- Sentences = list of sentences, where each sentence is a list of tokens (words).\n",
    "- size: Which corresponds to the embedding dimension, defaul=100.\n",
    "- window: The number of words before and after, default=5. (8 works better)\n",
    "- min_count: Ignores words that apper less than this number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same data as last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books=[open('./data/dorian.txt','r'),open('./data/earnest.txt','r'),\n",
    "       open('./data/essays.txt','r'),open('./data/ghost.txt','r'),\n",
    "       open('./data/happy_prince.txt','r'),open('./data/house_pomegranates.txt','r'),\n",
    "       open('./data/ideal_husband.txt','r'),open('./data/intentions.txt','r'),\n",
    "       open('./data/lady_windermere.txt','r'),open('./data/profundis.txt','r'),\n",
    "       open('./data/salome.txt','r'),open('./data/soul_of_man.txt','r'),\n",
    "       open('./data/woman_of_no_importance.txt','r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make it into one large corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" \".join([book.read() for book in books])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use sent_tokenizer from the nltk library to create the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_sentences = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with now make each raw sentence into a list of tokens, we use the word tokenizer from nltk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "for sentence in raw_sentences:\n",
    "    sentences+=[word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can train our first model by feeding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1=w2v.Word2Vec(sentences=sentences,size=40,window=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PAGE', 0.7386135458946228),\n",
       " ('VOICE', 0.7363929152488708),\n",
       " ('PLACE', 0.7285578846931458),\n",
       " ('JOKANAAN', 0.7023457288742065),\n",
       " ('THE', 0.6995697021484375),\n",
       " ('CAPPADOCIAN', 0.6701943278312683),\n",
       " ('SYRIAN', 0.6563177108764648),\n",
       " ('YOUNG', 0.6562599539756775),\n",
       " ('HOUSE', 0.6462533473968506),\n",
       " ('OTHER', 0.6446676254272461)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.most_similar([model_1['woman']-model_1['man']+model_1['Queen']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32195"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_2=w2v.Word2Vec(sentences=sentences[:15000],size=100,window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PGLAF', 0.7106172442436218),\n",
       " ('Brandon', 0.6801990866661072),\n",
       " ('ON', 0.6584423184394836),\n",
       " ('indicate', 0.6534081697463989),\n",
       " ('occur', 0.6410136222839355),\n",
       " ('OUT', 0.6010198593139648),\n",
       " ('HIM', 0.5976551175117493),\n",
       " ('Defects', 0.5816824436187744),\n",
       " ('linked', 0.5683529376983643),\n",
       " ('agree', 0.5644705891609192)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.most_similar([model_2['woman']-model_2['man']+model_2['Queen']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always keep training the model, try running the following line several times and note how the most similar words change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrid', 0.49167507886886597),\n",
       " ('pretty', 0.46648675203323364),\n",
       " ('evening', 0.4537237882614136),\n",
       " ('wore', 0.43760383129119873),\n",
       " ('inquired', 0.41685187816619873),\n",
       " ('Is', 0.38669198751449585),\n",
       " ('heard', 0.3799837827682495),\n",
       " ('sweet', 0.3729308247566223),\n",
       " ('sang', 0.3709564805030823),\n",
       " ('heavens', 0.36998653411865234)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.train(sentences)\n",
    "model_2.most_similar([model_2['woman']-model_2['man']+model_2['Queen']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many word2vec pretrained models out there, in what follows we load the google news one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line may take a while since it is putting about 3 Gigs on memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3=KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Queen', 0.8382776975631714),\n",
       " ('King', 0.5479298830032349),\n",
       " ('Mayfair_London_W1J', 0.5369688272476196),\n",
       " ('Queen_Elizabeth', 0.5142285823822021),\n",
       " ('Rockabilly_Wanda_Jackson', 0.5023638010025024),\n",
       " ('Mitzi_Sister', 0.4933725595474243),\n",
       " ('Lancashire_Regiment_QLR', 0.48999619483947754),\n",
       " ('Mean_Lisa_Lampanelli', 0.48064759373664856),\n",
       " ('Beehive_Hairdo', 0.48026183247566223),\n",
       " ('Osie_Ukwuoma', 0.47888004779815674)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.most_similar([model_3['Queen']-model_3['woman']+model_3['man']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can ask for more complex analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def a_b_c(a,b,c):\n",
    "    return model_3.most_similar([model_3[a]-model_3[b]+model_3[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 0.6023102402687073),\n",
       " ('sewage', 0.4261908531188965),\n",
       " ('turbid_water', 0.42406660318374634),\n",
       " ('brackish_groundwater', 0.42278411984443665),\n",
       " ('Water', 0.4190317392349243),\n",
       " ('Floridan_aquifer', 0.4161131978034973),\n",
       " ('CRMWD', 0.4132806062698364),\n",
       " ('aquifers', 0.4094894528388977),\n",
       " ('light', 0.4086752235889435),\n",
       " ('groundwater', 0.40861862897872925)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_b_c('light','lamp','water')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('petal', 0.7127969861030579),\n",
       " ('leaf', 0.7053060531616211),\n",
       " ('petals', 0.5032273530960083),\n",
       " ('petiole', 0.47272592782974243),\n",
       " ('circumferentially', 0.4649199843406677),\n",
       " ('lobed', 0.45781558752059937),\n",
       " ('nanorod', 0.4557533860206604),\n",
       " ('fernlike', 0.4547785222530365),\n",
       " ('sapwood', 0.45389315485954285),\n",
       " ('leaf_axils', 0.4537096619606018)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_b_c('petal','flower','leaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('human', 0.3660902976989746),\n",
       " ('mankind', 0.2857176959514618),\n",
       " ('humankind', 0.27630895376205444),\n",
       " ('humanity', 0.27607446908950806),\n",
       " ('macrocosm', 0.2735801935195923),\n",
       " ('profound', 0.2719401717185974),\n",
       " ('constrains', 0.2704422175884247),\n",
       " ('crossword_puzzling', 0.2655083239078522),\n",
       " ('littleness', 0.2646711766719818),\n",
       " ('perfections', 0.26055628061294556)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.most_similar([model_3['human']-model_3['animal']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the pretrained vectors from glove you should know that they are just in a txt file and they are a word followed by a space followed by the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    counter=0\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = np.array(embedding)\n",
    "        counter+=1\n",
    "        if counter>200000:\n",
    "            break\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 200001  words loaded!\n"
     ]
    }
   ],
   "source": [
    "model_4=loadGloveModel('./data/glove.42B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4['king'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use typical techniques to find distances, a pain to implement though"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
