{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now to use some pretrained embedding to see if we get better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import gensim.models.word2vec as w2v\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean the data as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_helpers import load_data_and_labels\n",
    "from tflearn.data_utils import VocabularyProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_len=60\n",
    "vocab_proc = VocabularyProcessor(sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sentences,y=load_data_and_labels('./data/rt/rt-polarity.pos','./data/rt/rt-polarity.neg')\n",
    "X = np.array(list(vocab_proc.fit_transform(X_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_proc.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The second model\n",
    "\n",
    "For our second model we use a pre-trained embedding. There are many options for this, let's use the word2vec model we train before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books=[open('./data/dorian.txt','r'),open('./data/earnest.txt','r'),\n",
    "       open('./data/essays.txt','r'),open('./data/ghost.txt','r'),\n",
    "       open('./data/happy_prince.txt','r'),open('./data/house_pomegranates.txt','r'),\n",
    "       open('./data/ideal_husband.txt','r'),open('./data/intentions.txt','r'),\n",
    "       open('./data/lady_windermere.txt','r'),open('./data/profundis.txt','r'),\n",
    "       open('./data/salome.txt','r'),open('./data/soul_of_man.txt','r'),\n",
    "       open('./data/woman_of_no_importance.txt','r'),open('./data/rt/rt-polarity.pos','r',encoding = \"ISO-8859-1\"),\n",
    "       open('./data/rt/rt-polarity.neg','r',encoding = \"ISO-8859-1\")]\n",
    "corpus = \" \".join([book.read() for book in books])\n",
    "raw_sentences = sent_tokenize(corpus)\n",
    "sentences=[]\n",
    "for sentence in raw_sentences:\n",
    "    sentences+=[word_tokenize(sentence)]\n",
    "emb_model=w2v.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's first get the vocabualry that we have in the movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list=list(vocab_proc.reverse([[i] for i in range(vocab_size)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we use the embedding matrix from the word2vec, but we need to build it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_matrix=np.array([emb_model[word] if word in emb_model else np.random.random(100) for word in vocab_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global hyper-parameters\n",
    "emb_dim=100\n",
    "hidden_dim=50\n",
    "num_classes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create the placeholders to hold the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.int32, shape=[None, sentence_len], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "#dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the variables we are going to need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"embedding\"):\n",
    "    W = tf.Variable(emb_matrix,name=\"W\",dtype=tf.float32)\n",
    "    embedded_chars = tf.nn.embedding_lookup(W, input_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put all the this vectors together into a large vector, for this we use the reshape method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"reshape\"):\n",
    "    emb_vec= tf.reshape(embedded_chars,shape=[-1,sentence_len*emb_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now go over the hidden dimension, but first we need a variable for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"hidden\"):\n",
    "    W_h= tf.Variable(tf.random_uniform([sentence_len*emb_dim, hidden_dim], -1.0, 1.0),name=\"w_hidden\")\n",
    "    b_h= tf.Variable(tf.zeros([hidden_dim],name=\"b_hidden\"))\n",
    "    hidden_output= tf.nn.relu(tf.matmul(emb_vec,W_h)+b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"output_layer\"):\n",
    "    W_o= tf.Variable(tf.random_uniform([hidden_dim,2], -1.0, 1.0),name=\"w_o\")\n",
    "    b_o= tf.Variable(tf.zeros([2],name=\"b_o\"))\n",
    "    score = tf.nn.relu(tf.matmul(hidden_output,W_o)+b_o)\n",
    "    predictions = tf.argmax(score, 1, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that we didn't put the softmax layer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    losses=tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=score)\n",
    "    loss=tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to start the session, we need the training operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer=tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_op=tf.summary.merge([loss_summary,acc_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_summary_writer = tf.summary.FileWriter('./summaries2/', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is step: 99, acc=0.50, loss=0.71\r"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    acc,loss_,_=sess.run([accuracy,loss,optimizer],feed_dict={input_x:X,input_y:y})\n",
    "    step,summaries = sess.run([global_step,summary_op],feed_dict={input_x:X,input_y:y})\n",
    "    train_summary_writer.add_summary(summaries, i)\n",
    "    print(\"This is step: %d, acc=%.2f, loss=%.2f\"%(i,acc,loss_),end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and after initiating tensorboard by using \n",
    "\n",
    "tensorboard --logdir=\"./summaries\"\n",
    "\n",
    "we can navigate to http://127.0.1.1:6006/ to see what we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework\n",
    "\n",
    "- Create a deeper model and compare its perfomance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
