{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first example of a deep NLP we create a simple (and naive) text classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "We use the [Movie Review data from Rotten Tomatoes](http://www.cs.cornell.edu/people/pabo/movie-review-data/) it consists on \n",
    "\n",
    "- 5331 Postive reviews.\n",
    "- 5331 Negative reviews.\n",
    "\n",
    "## Cleaning the data\n",
    "\n",
    "Before cleaning the data let's take a quick look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_helpers import load_data_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sentences,y=load_data_and_labels('./data/rt/rt-polarity.pos','./data/rt/rt-polarity.neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean claud van damme or steven segal\",\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sentences[0],y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('in comparison to his earlier films it seems a disappointingly thin slice of lower class london life despite the title amounts to surprisingly little',\n",
       " array([1, 0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sentences[6338],y[6338]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sentences may have different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[173,\n",
       " 216,\n",
       " 30,\n",
       " 87,\n",
       " 111,\n",
       " 133,\n",
       " 59,\n",
       " 106,\n",
       " 123,\n",
       " 73,\n",
       " 68,\n",
       " 83,\n",
       " 89,\n",
       " 67,\n",
       " 59,\n",
       " 121,\n",
       " 40,\n",
       " 38,\n",
       " 98,\n",
       " 55]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len,X_sentences))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fix this, and get our vocabulary, we can use methods from the tflearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.data_utils import VocabularyProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a constant to keep the sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_len=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_proc = VocabularyProcessor(sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(list(vocab_proc.fit_transform(X_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  1,  7,  8,  9, 10, 11, 12, 13, 14,  9, 15,\n",
       "        5, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, how the index 0 is been used to PAD. Let's look slightly more at this method, it produces a generator object which we can used to get the list corresponding to the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1, 8820,  415,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(vocab_proc.transform(['the reviews are']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also look at the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_proc.vocabulary_)\n",
    "#vocab_dict = vocab_processor.vocabulary_._mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first model\n",
    "\n",
    "For our first model we don't use a pre-trained embedding. Instead, we let the model learn the embedding by itself. \n",
    "\n",
    "After the embedding layer, we have a hidden layer and we end we some classification. We start by defining some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global hyper-parameters\n",
    "emb_dim=100\n",
    "hidden_dim=50\n",
    "num_classes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create the placeholders to hold the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.int32, shape=[None, sentence_len], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "#dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the variables we are going to need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([vocab_size, emb_dim], -1.0, 1.0),name=\"W\")\n",
    "    embedded_chars = tf.nn.embedding_lookup(W, input_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put all the this vectors together into a large vector, for this we use the reshape method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"reshape\"):\n",
    "    emb_vec= tf.reshape(embedded_chars,shape=[-1,sentence_len*emb_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now go over the hidden dimension, but first we need a variable for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"hidden\"):\n",
    "    W_h= tf.Variable(tf.random_uniform([sentence_len*emb_dim, hidden_dim], -1.0, 1.0),name=\"w_hidden\")\n",
    "    b_h= tf.Variable(tf.zeros([hidden_dim],name=\"b_hidden\"))\n",
    "    hidden_output= tf.nn.relu(tf.matmul(emb_vec,W_h)+b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"output_layer\"):\n",
    "    W_o= tf.Variable(tf.random_uniform([hidden_dim,2], -1.0, 1.0),name=\"w_o\")\n",
    "    b_o= tf.Variable(tf.zeros([2],name=\"b_o\"))\n",
    "    score = tf.nn.relu(tf.matmul(hidden_output,W_o)+b_o)\n",
    "    predictions = tf.argmax(score, 1, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that we didn't put the softmax layer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    losses=tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=score)\n",
    "    loss=tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to start the session, we need the training operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer=tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_op=tf.summary.merge([loss_summary,acc_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_summary_writer = tf.summary.FileWriter('./summaries/', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is step: 99, acc=0.51, loss=0.91\r"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    acc,loss_,_=sess.run([accuracy,loss,optimizer],feed_dict={input_x:X,input_y:y})\n",
    "    step,summaries = sess.run([global_step,summary_op],feed_dict={input_x:X,input_y:y})\n",
    "    train_summary_writer.add_summary(summaries, i)\n",
    "    print(\"This is step: %d, acc=%.2f, loss=%.2f\"%(i,acc,loss_),end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and after initiating tensorboard by using \n",
    "\n",
    "tensorboard --logdir=\"./summaries\"\n",
    "\n",
    "we can navigate to http://127.0.1.1:6006/ to see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
