{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we implement a simple Deep Learning dependency parser following this [paper](http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf) and the [homework](http://web.stanford.edu/class/cs224n/assignment2/index.html) from the Standford NLP course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a dependency parser?\n",
    "\n",
    "Given a sentence organize the words in such a way that we know which words are arguments of other words. This are called dependencies.\n",
    "\n",
    "### Example\n",
    "\n",
    "How should we parse the sentence \"I saw an ice-cream with my phone\"?\n",
    "\n",
    "'saw'-> 'I'\n",
    "\n",
    "'saw'-> 'ice-cream'\n",
    "\n",
    "'saw'-> 'with'\n",
    "\n",
    "'ice-cream' -> 'an'\n",
    "\n",
    "'with' -> 'phone'\n",
    "\n",
    "'phone'-> 'my'\n",
    "\n",
    "How can we build our own parser?\n",
    "\n",
    "## Greedy Transition-Based parsing\n",
    "\n",
    "Let $S=w_0w_1\\cdots w_n$ be a sentence, we consider the triple $$ c=(\\gamma,\\beta, A),$$ where\n",
    "\n",
    "- A stach $\\gamma$ of words from $S$.\n",
    "- A buffer $\\beta$ of words from $S$.\n",
    "- A **set** of arcs $A$ of the form $(w_i,w_j)$.\n",
    "\n",
    "The **goal** is to start from the inital state\n",
    "\n",
    "$$c_0=\\left(\\gamma=['ROOT'],\\beta=[w_0,w_1,\\ldots,w_n],A=\\emptyset\\right)$$\n",
    "\n",
    "and use the transitions \n",
    "\n",
    "- **Shift**: Remove first words in $\\beta$ and push it on top of $\\gamma$.\n",
    "- **Left-Arc**: Add $(w_j,w_i)$ to $A$, where $w_i$ is the second to the top word on $\\gamma$ and $w_j$ is the word at the top. Then, remove $w_i$ from $\\gamma$.\n",
    "- **Right-Arc**: Add $(w_i,w_j)$ to $A$, where $w_i$ is the second to the top word on $\\gamma$ and $w_j$ is the word at the top. Then, remove $w_j$ from $\\gamma$.\n",
    "\n",
    "to achieve a configuration\n",
    "\n",
    "$$c_0=\\left(\\gamma=['ROOT'],\\beta=\\emptyset,A=\\right)$$\n",
    "\n",
    "where A is now a parsing structure for the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partialParse Object\n",
    "\n",
    "We start by creating an object to keep the different configurations that we obtain from c, we call that object a partialParse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Initializes this partial parse.\n",
    "            We initialize the following fields:\n",
    "            self.stack: The current stack represented as a list with the top of the stack as the\n",
    "                        last element of the list.\n",
    "            self.buffer: The current buffer represented as a list with the first item on the\n",
    "                         buffer as the first item of the list\n",
    "            self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "                    tuples where each tuple is of the form (head, dependent).\n",
    "                    Order for this list doesn't matter.\n",
    "\n",
    "        The root token should be represented with the string \"ROOT\"\n",
    "\n",
    "        Args:\n",
    "            sentence: The sentence to be parsed as a list of words.\n",
    "                      Your code should not modify the sentence.\n",
    "        \"\"\"\n",
    "        # The sentence being parsed is kept for bookkeeping purposes.\n",
    "        \n",
    "        self.sentence = sentence\n",
    "        self.stack=['ROOT']\n",
    "        self.buffer=sentence\n",
    "        self.dependencies=[]\n",
    "       \n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
    "\n",
    "        Args:\n",
    "            transition: A string that equals \"S\", \"LA\", or \"RA\" representing the shift, left-arc,\n",
    "                        and right-arc transitions.\n",
    "        \"\"\"\n",
    "        \n",
    "        if transition==\"S\":\n",
    "            # Removes the first one from the buffer\n",
    "            b1=self.buffer.pop(0)\n",
    "            # adds it to the stack\n",
    "            self.stack.append(b1)\n",
    "            \n",
    "        elif transition==\"LA\":\n",
    "            #removes the second one from the stack\n",
    "            s2=self.stack.pop(-2)\n",
    "            \n",
    "            s1=self.stack[-1]\n",
    "            #adds the arc s1->s2 to the dependencies.\n",
    "            self.dependencies.append((s1,s2))\n",
    "            \n",
    "        elif transition==\"RA\":\n",
    "            #removes the first one from the stack\n",
    "            s1=self.stack.pop(-1)\n",
    "            \n",
    "            s2=self.stack[-1]\n",
    "            #adds the arc s2->s1 to the dependencies.\n",
    "            self.dependencies.append((s2,s1))\n",
    "\n",
    "\n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "\n",
    "        Args:\n",
    "            transitions: The list of transitions in the order they should be applied\n",
    "        Returns:\n",
    "            dependencies: The list of dependencies produced when parsing the sentence. Represented\n",
    "                          as a list of tuples where each tuple is of the form (head, dependent)\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example if we have the sentence \"I saw a cow\", we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('saw', 'I'), ('cow', 'a'), ('saw', 'cow')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pp=PartialParse(['I','saw','a','cow'])\n",
    "example_pp.parse(['S','S','LA','S','S','LA','RA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Check the transitions that are necessary to give a correct parsing of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The deep learning stuff\n",
    "\n",
    "In order to create such a parser we follow the approach of assigment 2. That is, we want to create a NN that for a given triple $c$ decides what is the next transition. \n",
    "\n",
    "We are given some training data, for our purposes the data looks like a collection of tables like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|index|word|arrow from|\n",
    "|---|---|---|\n",
    "|1|Ms.|2|\n",
    "|2|Haag|3|\n",
    "|3|plays|0|\n",
    "|4|Elianti|3|\n",
    "|5|.|3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it will be preprocessed so it looks in the form X,y. We will go over the generalities of this during the lecture. But keep in mind that a greedy approach in the selection of the correct arcs makes the desired steps unique. \n",
    "\n",
    "In the homework, the method that gives the data is load_and preprocess_data inside the parser_utils module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assignment2_sol.utils.parser_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "took 3.28 seconds\n",
      "Building parser...\n",
      "took 0.09 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 3.42 seconds\n",
      "Vectorizing data...\n",
      "took 0.10 seconds\n",
      "Preprocessing training data...\n",
      "1000/1000 [==============================] - 5s     \n"
     ]
    }
   ],
   "source": [
    "data=load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we obtain 5 different objects, we only care about the second one which is the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix=data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the third one who is the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "furthermore, the training data comes in the form of a list which each elemnt is (x,something,y), so we collect the x and the y parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_input_data=np.array([train_data_element[0] for train_data_element in train_data])\n",
    "y_input_data_=np.array([train_data_element[2] for train_data_element in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need one more preprocessing since the y_input_data is not exactly in the shape we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_input_data = np.zeros((y_input_data_.size, 3))\n",
    "y_input_data[np.arange(y_input_data_.size), y_input_data_] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to build our model, first some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 36\n",
    "n_classes = 3\n",
    "dropout = 0.5\n",
    "embed_size = 50\n",
    "hidden_size = 200\n",
    "batch_size = 2048\n",
    "n_epochs = 10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we start with the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_placeholder=tf.placeholder(shape=(None,n_features),dtype=tf.int32,name='input_placeholder')\n",
    "labels_placeholder=tf.placeholder(shape=(None,n_classes),dtype=tf.float32,name='labels_placeholder')\n",
    "dropout_placeholder=tf.placeholder(shape=(),dtype=tf.float32,name='dropout_rate_placeholder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a dictionary to feed the data to the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict={input_placeholder: x_input_data,\n",
    "           labels_placeholder:y_input_data,\n",
    "           dropout_placeholder: dropout}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings=tf.Variable(emb_matrix)\n",
    "embeddings= tf.nn.embedding_lookup(embeddings,input_placeholder)\n",
    "embeddings= tf.reshape(embeddings,shape=(-1,embed_size*n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then the prediction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=tf.Variable(tf.random_normal(shape=(n_features*embed_size,hidden_size)))\n",
    "b1=tf.Variable(tf.zeros(hidden_size))\n",
    "U = tf.Variable(tf.random_normal(shape=(hidden_size,n_classes)))\n",
    "b2=tf.Variable(tf.zeros(n_classes))\n",
    "\n",
    "pred = tf.nn.relu(tf.matmul(embeddings,W)+b1)\n",
    "\n",
    "pred = tf.nn.dropout(pred,dropout)\n",
    "\n",
    "pred = tf.matmul(pred,U)+b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=labels_placeholder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and something to measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.argmax(pred, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(labels_placeholder, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the training op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op=tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is step: 99, acc=0.68, loss=64.971\r"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    acc,loss_,_=sess.run([accuracy,loss,train_op],feed_dict=feed_dict)\n",
    "    print(\"This is step: %d, acc=%.2f, loss=%.2f\"%(i,acc,loss_),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
